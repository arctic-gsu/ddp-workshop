{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f416595d-5c68-4cdb-bacd-b91935ea7597",
   "metadata": {},
   "source": [
    "## Import Libraries </br>\n",
    "torch - deep learning framework </br>\n",
    "torch functional - torch API</br>\n",
    "Dataset - Dataset stores the samples and their corresponding labels</br>\n",
    "DataLoader - DataLoader wraps an iterable around the Dataset to enable easy access to the samples, iterable over a dataset </br>\n",
    "MyTrainDataset - Custom class in datautils file</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29eee8c-6395-4203-abc0-389106b9451c",
   "metadata": {},
   "source": [
    "## Make a custom dataset class. \n",
    "\n",
    "Datasets represent a map from keys to data samples.\n",
    "All subclasses overwrite `__getitem__`, which supports fetching a data sample for a given key.\n",
    "And, __len__ function counts the size of total sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295b909e-d59e-4f14-93a5-913317faee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyTrainDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.data = [(torch.rand(20), torch.rand(1)) for _ in range(size)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15ddeda0-707c-47d5-91c5-577a0b29882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5371ac-e4b9-4183-ba51-c01bdd6e789a",
   "metadata": {},
   "source": [
    "### Class Trainer\n",
    "\n",
    "\n",
    "We create a class Trainer to train our network. <br>\n",
    "The class variables are:<br>\n",
    "gpu_id: gpu number <br>\n",
    "model: model used for training<br>\n",
    "train_data: dataset for training<br>\n",
    "optimizer: optimizer for gradients<br>\n",
    "save_every: integer for stating number<br>\n",
    "\n",
    "### Class functions:<br>\n",
    "#### _run_batch:\n",
    "    def _run_batch(self, source, targets):\n",
    "        This function is used to run a batch job, the arguments are source and targets.\n",
    "        self.optimizer.zero_grad() - makes all the gradient to zero\n",
    "        self.model - calling source with model class\n",
    "        F.cross_entropy - calculating loss function\n",
    "        loss.backward -  to perform backpropagation: Computes the Gradient: It calculates the gradients of the loss function with respect to the model parameters (weights and biases).  Clearing gradients should be done by optimizer.zero_grad() before calling this\n",
    "        Once the gradients are computed and stored, they can be used by an optimizer\n",
    "        self.optimizer.step() -  is a method used in PyTorch that applies the gradients computed from loss.backward() to update the model's parameters\n",
    "\n",
    "#### _run_epoch\n",
    "    def _run_epoch(self, epoch):\n",
    "        epoch: represents the current training epoch number\n",
    "        b_sz: calculates the batch size used for training\n",
    "        for source, targets in self.train_data: starts a loop over self.train_data, which is an iterable yielding batches of data\n",
    "        source: source.to(self.gpu_id): moves the source tensor to the GPU specified by self.gpu_id\n",
    "        targets: Similar to the previous line, this line moves the targets tensor to the GPU\n",
    "        self._run_batch(source, targets): calls class _run_batch\n",
    "\n",
    "\n",
    "#### _save_checkpoint\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        loads and saves checkpoint\n",
    "\n",
    "#### train\n",
    "    begins calling classes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88b2da5-4ed2-4f74-9ce9-77bbba8d8cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        train_data: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        gpu_id: int,\n",
    "        save_every: int, \n",
    "    ) -> None:\n",
    "        self.gpu_id = gpu_id\n",
    "        self.model = model.to(gpu_id)\n",
    "        self.train_data = train_data\n",
    "        self.optimizer = optimizer\n",
    "        self.save_every = save_every\n",
    "\n",
    "    def _run_batch(self, source, targets):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(source)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _run_epoch(self, epoch):\n",
    "        b_sz = len(next(iter(self.train_data))[0])\n",
    "        print(f\"[GPU{self.gpu_id}] Epoch {epoch} | Batchsize: {b_sz} | Steps: {len(self.train_data)}\")\n",
    "        for source, targets in self.train_data:\n",
    "            source = source.to(self.gpu_id)\n",
    "            targets = targets.to(self.gpu_id)\n",
    "            self._run_batch(source, targets)\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        ckp = self.model.state_dict()\n",
    "        PATH = \"checkpoint.pt\"\n",
    "        torch.save(ckp, PATH)\n",
    "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
    "\n",
    "    def train(self, max_epochs: int):\n",
    "        for epoch in range(max_epochs):\n",
    "            self._run_epoch(epoch)\n",
    "            if epoch % self.save_every == 0:\n",
    "                self._save_checkpoint(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb8cf8d-229d-47eb-b88f-d5ee95a51fc2",
   "metadata": {},
   "source": [
    "### load_train_objs\n",
    "\n",
    "MyTrainDataset is a custom dataset class designed to hold or generate training data.<br>\n",
    "###### model = torch.nn.Linear(20, 1):<br>\n",
    "    This line initializes a linear model (model) using PyTorch's torch.nn.Linear class, which creates a linear transformation from an input size of 20 to an output size of 1<br>\n",
    "###### optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)<br>\n",
    "    An optimizer object named optimizer is created using PyTorch's Stochastic Gradient Descent (SGD) optimization algorithm, using a learning rate (lr) of 0.001 (1e-3). The optimizer is responsible for updating the model's weights based on the gradients computed during the training process to minimize the loss function.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "467477e2-1fde-4bcb-9a89-95a60765833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_objs():\n",
    "    train_set = MyTrainDataset(2048)  # load your dataset\n",
    "    model = torch.nn.Linear(20, 1)  # load your model\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "    return train_set, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353d0f2-3791-4b38-9517-a74c0634de51",
   "metadata": {},
   "source": [
    "## Preparing dataloader \n",
    "The prepare_dataloader function is designed to wrap a given dataset into a PyTorch DataLoader, which is a powerful utility that provides an iterable over the given dataset. It abstracts the complexity of batching, shuffling, and loading the data in parallel with the use of workers. Here's an explanation of each part of the function:\n",
    "\n",
    "###### dataset\n",
    "\n",
    "    The first argument to the DataLoader constructor is the dataset itself. This is the dataset object from which data will be loaded.\n",
    "\n",
    "###### batch_size=batch_size,\n",
    "\n",
    "    This specifies the size of the batch that the DataLoader will return with each iteration. The batch_size argument determines how many data points are included in each batch. This is useful for specifying how much data your model will process in one forward/backward pass.\n",
    "\n",
    "###### pin_memory=True,\n",
    "\n",
    "    Setting pin_memory=True is a performance optimization that can lead to faster data transfer to CUDA-enabled GPUs by having the data loader allocate the samples in page-locked (or \"pinned\") memory. This setting is beneficial when using GPUs for training because it allows for more efficient transfer of data from host (CPU) to device (GPU) memory.\n",
    "\n",
    "###### shuffle=True\n",
    "\n",
    "    This argument specifies that the data should be shuffled at the beginning of each epoch. Shuffling the data is important for preventing the model from learning anything from the order of the samples, and thus, it helps to reduce overfitting and ensures that the model generalizes well.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a43bb38-64d5-4a6b-a384-dce360108b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(dataset: Dataset, batch_size: int):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283a71b-b369-48fa-9a86-d36b93b36060",
   "metadata": {},
   "source": [
    "## The main function\n",
    "\n",
    "###### dataset, model, optimizer = load_train_objs()\n",
    "\n",
    "    Calls the load_train_objs function, which is expected to return a tuple containing three elements: the dataset (dataset), the model (model), and the optimizer (optimizer). This line unpacks the returned tuple into the three variables.\n",
    "\n",
    "###### train_data = prepare_dataloader(dataset, batch_size)\n",
    "\n",
    "    Invokes the prepare_dataloader function with the previously loaded dataset and the specified batch_size. This function returns a DataLoader object, which is assigned to train_data. The DataLoader provides an iterable over the dataset, automatically handling batching, shuffling, and the potential parallel loading of data.\n",
    "\n",
    "###### trainer = Trainer(model, train_data, optimizer, device, save_every)\n",
    "\n",
    "    Instantiates a Trainer object with the model, train_data DataLoader, optimizer, device, and save frequency (save_every). This Trainer class is not defined in the provided code but is presumably designed to encapsulate the training loop, handling the training process based on the given parameters.\n",
    "\n",
    "###### trainer.train(total_epochs)\n",
    "\n",
    "    Calls the train method of the Trainer object with total_epochs as its argument. This method likely executes the training loop, iterating over the dataset for the specified number of epochs, performing forward and backward passes, updating the model's weights, and possibly saving the model's state at intervals defined by save_every."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2a07f6d-c045-4ae1-b1ea-3df56e1b1558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(device, total_epochs, save_every, batch_size):\n",
    "    dataset, model, optimizer = load_train_objs()\n",
    "    train_data = prepare_dataloader(dataset, batch_size)\n",
    "    trainer = Trainer(model, train_data, optimizer, device, save_every)\n",
    "    trainer.train(total_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84002f6c-f898-42e1-953e-4eea6cf8d6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU0] Epoch 0 | Batchsize: 32 | Steps: 64\n",
      "Epoch 0 | Training checkpoint saved at checkpoint.pt\n",
      "[GPU0] Epoch 1 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 2 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 3 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 4 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 5 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 6 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 7 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 8 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 9 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 10 | Batchsize: 32 | Steps: 64\n",
      "Epoch 10 | Training checkpoint saved at checkpoint.pt\n",
      "[GPU0] Epoch 11 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 12 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 13 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 14 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 15 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 16 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 17 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 18 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 19 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 20 | Batchsize: 32 | Steps: 64\n",
      "Epoch 20 | Training checkpoint saved at checkpoint.pt\n",
      "[GPU0] Epoch 21 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 22 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 23 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 24 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 25 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 26 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 27 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 28 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 29 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 30 | Batchsize: 32 | Steps: 64\n",
      "Epoch 30 | Training checkpoint saved at checkpoint.pt\n",
      "[GPU0] Epoch 31 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 32 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 33 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 34 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 35 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 36 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 37 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 38 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 39 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 40 | Batchsize: 32 | Steps: 64\n",
      "Epoch 40 | Training checkpoint saved at checkpoint.pt\n",
      "[GPU0] Epoch 41 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 42 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 43 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 44 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 45 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 46 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 47 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 48 | Batchsize: 32 | Steps: 64\n",
      "[GPU0] Epoch 49 | Batchsize: 32 | Steps: 64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = 0  # shorthand for cuda:0\n",
    "    main(device, total_epochs=50, save_every=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e29d5-efa9-42b7-a279-14538e2828f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdc787-3547-4fe3-a3d3-a72801305b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b098d1-a9c4-4d42-a6a8-e00fbac53573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be0231-5dad-40fa-9a2f-4cdb1d735ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
